{
  "hash": "2994d913964951113d31f52683e8f310",
  "result": {
    "markdown": "---\ntitle: \"Blog post 6\"\nsubtitle: \"Current model analysis and next step discussion\"\ndescription: \"\"\nauthor: \"Team 9\"\ndate: \"2023-12-04\"\nimage: \"\"\nimage-alt: \"\"\ncategories: []\nformat: gfm\nhtml_document: \nself_contained: false\ndraft: FALSE\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"C:/Users/pfwan/OneDrive/Desktop/MA 615/ma-4615-fa23-final-project-burg-team-9/dataset/traffic_violations.RData\")\n\nmydata <- traffic_data_clean\n\n#choose Arrest types as our binomial dependent variables, which 'marked' arrest type as 1 and 'Unmarked' arrest type as 0#\nmydata$Arrest_Type <- ifelse(grepl(\"Marked\", traffic_data_clean$`Arrest Type`), 1, 0)\n\nx1 <- mydata$Race\nx2 <- mydata$Alcohol\nx3 <- mydata$Gender\n\nX <- data.frame(x1,x2,x3)\nY <-mydata$Arrest_Type\n\n#Build Train and Test datasets#\n\n\nsplit_index <- sample(1:length(x1), size = floor(0.8 * length(x1)))\n\n\nx_train <- X[split_index, ]\nx_test <- X[-split_index, ]\ny_train <- Y[split_index]\ny_test <- Y[-split_index]\n\n\n#initialize the original binomial linear regression, choose Race, Alcohol and Gender as independent variables with x_train datasets#\n\nmodel <- glm(y_train ~ ., data = cbind(y_train, x_train), family = \"binomial\")\n \nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y_train ~ ., family = \"binomial\", data = cbind(y_train, \n    x_train))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.6833   0.3628   0.3740   0.4105   1.2526  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        2.725067   0.014854 183.460  < 2e-16 ***\nx1BLACK           -0.105202   0.015386  -6.838 8.05e-12 ***\nx1HISPANIC        -0.042186   0.015960  -2.643  0.00821 ** \nx1NATIVE AMERICAN  0.249655   0.090690   2.753  0.00591 ** \nx1OTHER           -0.459548   0.018239 -25.196  < 2e-16 ***\nx1WHITE           -0.298661   0.015161 -19.699  < 2e-16 ***\nx2Yes             -2.440670   0.044489 -54.861  < 2e-16 ***\nx3M                0.005286   0.006719   0.787  0.43144    \nx3U                0.847250   0.104877   8.079 6.56e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 788248  on 1508357  degrees of freedom\nResidual deviance: 783929  on 1508349  degrees of freedom\nAIC: 783947\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\nIn the original model we generated, we have observed that except the variable \"male Gender\", every other categorical variables has significant effect in the prediction of probability of our dependent model, which points out that there is relation between the categorical variables we choose and their arrest type probability. \n\nWhat's more, based on the coefficient value, we can summarize that the variables \"Black\",\"Hispanic\",\"Other\" and \"white\" in Categorical \"Race\", the variable \"Alcohol\" has negative log-odds, which means that will lower the probability they got arrest type as \"marked\". While the variables \"Male\" \"U\" in Categorical variable\"Gender\", the Race \"Native American\" has positive log-odds, which will increase their probabiliry they got the arrest type as \"Marked\". \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Build Feature Importance#\n\ncoefficients <- coef(model)\nvariable_names <- names(coefficients)\n\nimportances <- data.frame(\n  'Attribute' = variable_names,\n  'Importance' = coefficients\n)\n\nlibrary(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.3.1\n```\n:::\n\n```{.r .cell-code}\nggplot(importances, aes(x = Attribute, y = Importance, fill = Importance > 0)) +\n  geom_bar(stat = \"identity\", color = \"lightblue\") +\n  scale_fill_manual(values = c(\"lightgreen\", \"yellow\")) + \n  labs(title = \"Feature importances obtained from coefficients\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Attribute\") +\n  ylab(\"Importance\")\n```\n\n::: {.cell-output-display}\n![](blog-post-6_files/figure-commonmark/unnamed-chunk-2-1.png)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Build Confusion matrix, find accuracy#\n\nY_pred <- predict(model, newdata = cbind(y_test, x_test), type = \"response\")\n#we choose the 0.92 as the diff of our predict outcome#\nY_pred_binary <- ifelse(Y_pred > 0.5, 1, 0)\n\naccuracy <- mean(Y_pred_binary == y_test)\n\nprint(accuracy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9273277\n```\n:::\n\n```{.r .cell-code}\ntable(Y_pred_binary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nY_pred_binary\n     0      1 \n   249 376841 \n```\n:::\n\n```{.r .cell-code}\nprint(sum(Y_pred_binary == 0)/sum(Y_pred_binary==1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0006607561\n```\n:::\n\n```{.r .cell-code}\nprint(sum(Y == 0)/sum(Y==1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.07869144\n```\n:::\n:::\n\nOur model accuracy is 0.9277944, which roughly is %92.78, when we pick the predict probability larger than 0.5 as 1 and less than 0.5 as 0. Compare with the marked and unmarked arrest type proportion, we may cautiously evaluate this model's accuracy because we have to think about the probability of overfitting in our mode and the imbalance in our model.   \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Confusion Matrix#\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'caret' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_matrix <- confusionMatrix(factor(Y_pred_binary), factor(y_test))\nprint(conf_matrix)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction      0      1\n         0    151     98\n         1  27306 349535\n                                          \n               Accuracy : 0.9273          \n                 95% CI : (0.9265, 0.9282)\n    No Information Rate : 0.9272          \n    P-Value [Acc > NIR] : 0.3714          \n                                          \n                  Kappa : 0.0096          \n                                          \n Mcnemar's Test P-Value : <2e-16          \n                                          \n            Sensitivity : 0.0054995       \n            Specificity : 0.9997197       \n         Pos Pred Value : 0.6064257       \n         Neg Pred Value : 0.9275397       \n             Prevalence : 0.0728129       \n         Detection Rate : 0.0004004       \n   Detection Prevalence : 0.0006603       \n      Balanced Accuracy : 0.5026096       \n                                          \n       'Positive' Class : 0               \n                                          \n```\n:::\n:::\n\n\n\nIn the above summary, we have notices that the null deviance is 984980 and residual deviance is 979510, and the deviance range is (-2.6691,1.2755), and our model may contains over fitting because of the imbalance outcome proportion in our dependent variable. \n\nBased on the current outcome of our origional model, to enhance the performance of our original model, we are exploring several avenues for improvement.  First and foremost, our current dataset comprises solely categorical variables.  Introducing relevant numerical variables could significantly augment our model's predictive capabilities, particularly when utilizing logistic regression as our analytical framework.  To achieve this, we are considering merging our existing dataset with other correlated datasets, a step we believe is crucial in uncovering additional potential relationships between the dependent and independent variables.\n\nAdditionally, we recognize the imbalance in our current dependent variable.  To address this, we are contemplating the exploration of alternative dependent variables.  Building multiple analysis models with varied dependent variables can provide a broader perspective, potentially revealing different relationships and leading to more robust conclusions.  This approach not only enhances the stability of our findings but also enriches our exploratory data analysis (EDA) by fostering new insights.\n\nBy diversifying our dataset and exploring alternative dependent variables, we aim to refine our understanding of the underlying relationships, ultimately improving the overall performance and reliability of our analytical models.  \n\nWe plan to polish our visualizations and tables after finalizing our model to best show the data in terms of our method of statistical modeling. This will include highlighting information that informed the variables we made the decision to include. For example, in our scatterplot from our exploratory data analysis we will highlight the variables race, alcohol, and gender which seemed to have a potential relationship with arrest_type_numeric to show that this contributed to these variables being chosen for our statistical model. We also plan to add titles to our visualizations and tables to more clearly show what is being modeled in that particular plot. \n\nAdditionally, we will polish up our data visualizations by adding captions and annotations to write a short summary of what our takeaways from that visualization were. For example, in the scatterplot mentioned before we will add a caption or annotation saying that only race, alcohol, and gender appear to potentially have a relationship with arrest_type_numeric. We also plan to improve our figures using the options for displaying tables from https://gallery.htmlwidgets.org/, particularly the scatterD3 option to include both colors and comments more clearly for specific points in our plots. We are also planning to use pairsD3 to be able to show various relationships between variables that we explored to choose our model. \n\nWe are still trying out different EDA, with GLM being the priority. We are still unsatisfied with produced results, but we believe we could reach a conclusion that everyone can agree.\n\n\n\n\n",
    "supporting": [
      "blog-post-6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}