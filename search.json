[
  {
    "objectID": "posts/2023-10-30-blog-post-2/blog-post-2.html",
    "href": "posts/2023-10-30-blog-post-2/blog-post-2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "Since we are working with a large data set, we needed to start with only a subset of the data. We chose this subset by including columns without common instances of missing data. We also focused on choosing columns with different variables instead of columns that generally had the same response for each row in order to be able to see more variation in patterns and trends. This managed to create a subset that is now 34.6 MB so it will be more efficient to work with, but we will be able to add in more data later in the process if needed.\nThe principles for advancing equitable data practice that are relevant to our data are mainly beneficence and justice. Beneficence is relevant because it involves data that is collected with identification. Adhering to this practice involves removing anything in our data that could be used to identify people. In order to this, we remove any violation numbers or record numbers that could be traced back to a specific person. Justice is relevant because it involves considering the community in our design interest. Adhering to this practice involves us considering how the specific community from which the data comes from will impact our analysis and keeping that in mind instead of generalizing our data.\nOne of the important principles that were discussed was transparency. In order to implement this, we will need to be transparent both about what the limits of the data we have chosen are and what data informs our analysis. This will involve documenting our process clearly along the way in terms of how we are making this analysis. Therefore, some limitations of our analysis will be that we can only create analysis in terms of the specific community that our data comes from rather than generalizing it. We will also have to be aware that there may have been biases in how the data was collected so it may not be completely accurate or representative. For example, the data recorded at night may have been less accurate than the observations collected during the day. Additionally, observations may have been more likely to be recorded for certain types of violations, for example, ones that were worse, or for certain race categories.\nTransparency also involves being clear and transparent about what the plans are for the data after the project concludes. This refers to the potential for abuse or misuse of the data if we are not cautious about who has access to the data both during and after the project is completed."
  },
  {
    "objectID": "posts/2023-12-11-blog-post-7/blog-post-7.html",
    "href": "posts/2023-12-11-blog-post-7/blog-post-7.html",
    "title": "Blog Post 7",
    "section": "",
    "text": "Our tentative thesis statement is that non-white populations of Montgomery County, Maryland have more instances of traffic violations when compared to areas of the county that are predominantly white. After our exploratory data analysis, we noticed patterns between race dispersity and location from our longitude and latitude data columns. This became more evident in our plots showing the map of Montgomery County, in which we colored the map by the proportion of the population that is of a certain race category. Since we split this map into separate maps for each race category in our dataset, we were able to see that when we overlaid points for where the traffic violations occurred, the clusters of traffic violation incidents were generally in and around the areas where the proportion of the population that is white was the lowest, and the proportion of the population that is of different race categories is higher. Therefore, we included race as a necessary variable in our statistical model for predicting arrest types.\nWe also included alcohol and gender to improve the model’s accuracy. We chose these predictor variables to include from our exploratory data analysis as well. We could see a pattern indicating that alcohol may be related to arrest type, as well as a trend that appeared to show gender being related to arrest type as well. We additionally used AIC values to determine groupings of predictor variables to find the best fitting model for our data. Because the model using race, alcohol, and gender to predict arrest type had the lowest AIC, it confirmed to us that these variables should be included in our statistical model.\nTherefore, both our exploratory data analysis and our model center around our thesis, which states that traffic violations are more likely to occur in areas of Montgomery County, Maryland that have higher proportions of the population as racial categories other than white. After our exploratory data analysis brought these patterns and trends to our attention, we were able to create a glm model for the binomial variable arrest type. While we will continue to improve our model’s accuracy for our next steps, we aim to use this model to show our traffic violation arrest types vary by location based on the proportion of the population that is non-white, showing how different violations are more likely to occur when the highest proportion of the population of an area is non-white."
  },
  {
    "objectID": "posts/2023-11-13-blog-post-4/blog-post-4.html",
    "href": "posts/2023-11-13-blog-post-4/blog-post-4.html",
    "title": "Blog Post 4",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nload(here::here(\"dataset/traffic_violations.RData\"))\n \nwd &lt;- traffic_data_clean %&gt;% \n  rename(Arrest_Type = `Arrest Type`)\n \n# Race vs Arrest Type\nggplot(wd, aes(x = Race, fill = Arrest_Type)) + \n  geom_bar(position = \"dodge\") + \n  labs(title = \"Distribution of Race by Arrest Type\", \n       x = \"Race\", y = \"Count\",\n       fill = \"Arrest Type\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\nggplot(wd, aes(x = Race, fill = Accident)) + \n  geom_bar(position = \"dodge\") + \n  labs(title = \"Distribution of Race by Accident\", \n       x = \"Race\", y = \"Count\",\n       fill = \"Accident\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n# Statistical Modeling\n\nlibrary(tidyverse)\n\ntraffic_data_clean &lt;- traffic_data_clean |&gt;\n  mutate(Race_binary = ifelse(Race == \"Hispanic\", 1, 0))\n\nfiltered_data &lt;- traffic_data_clean |&gt;\n  filter(Year &gt;= 2000 & Year &lt;= 2023) |&gt;\n  mutate(Numbered_Year = Year - 2000)\n\nmodel &lt;- glm(Race_binary ~ Numbered_Year, family = binomial, data = filtered_data)\n\nWarning: glm.fit: algorithm did not converge\n\nggplot(filtered_data, aes(x = Numbered_Year, y = Race_binary)) +\n  geom_point() +\n  geom_smooth(method = 'glm', formula = y ~ x, se = FALSE, color = \"blue\", size = 0.5) +\n  labs(title = \"Logistic Regression: Race_binary ~ Numbered_Year\",\n       x = \"Numbered_Year\",\n       y = \"Race_binary\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFrom our last blog post, we saw that one of the most big, obvious trends was that the Driver’s License State category was predominantly from Maryland. Since this is to be expected given that the data is from a county in Maryland, this may be a boring part of the data. So, we chose to explore some other relationships that might be confounded by this major trend we identified.\nTo explore our data further in this way, we first looked at relationships among different variables and groupings by looking for trends or patterns between race categories and arrest types as well as race categories and accident types. We then focused on developing a better understanding of the specific relationship between race and arrest types and race and accident types by also including the time of day to see if this too affects the outcome of the traffic violation.\nBeginning to think about statistical modeling, the response variable that we are interested in is the outcome of the traffic violation, which in our data is represented as arrest type and accident type. The predictor variables we are interested in are race and time of day. To include time of day we would need to perform some transformations to better capture the relationships between the variables. Since time is going in a loop as it goes to the end of the day and then the values go back down to 0, we need to transform this column of the data to be continuous instead of a cycle. We will fit a linear model so that we can see how the predictor variables affect the response variable. For this, we will also need another transformation to represent the outcome of the traffic violation as binary variables. For the arrest type, we can choose “A – Marked Patrol” to be represented by 1, and each other arrest type to be 0. Similarly, for the accident column, we can choose “Yes” to be 1 and “No” to be 0. After these transformations, we can use the categorical variables in the linear model.\nFor our initial statistical modeling, we began by using a logistic model with the race as the response variable and the year as the predictor variable to observe the potential relationships between the differences in race categories reported for traffic violations over a number of years. For this initial modeling, we transformed the race variable to be binary by mutating “Hispanic” to be 1 and every other race category to be 0. We also transformed the Year variable to be represented as a value showing how many years from 2000 it is. We also only included the years 2000-2023 to first see how we could analyze a subset of the data on a smaller scale. For our future statistical modeling, we will also include the other race categories to see the trends and patterns between the different categories and include more years to see more widespread trends. This initial model showed a straight horizontal line after the regression, so in our further analysis we will additionally add more variables to find a model that reveals more about the data and its trends."
  },
  {
    "objectID": "posts/2023-10-20-blog-post-1/blog-post-1.html",
    "href": "posts/2023-10-20-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "Traffic and Drugs Related Violations Dataset https://www.kaggle.com/datasets/shubamsumbria/traffic-violations-dataset/data This data set has 15 columns and 52,967 rows. The data was originally collected to record observations of traffic and drugs violations. This dataset is in the form of a CSV file, so we will be able to load and clean the data. The main question we hope to address is if one race has more traffic and drugs related violations than the other race categories. Another question we hope to address is if the outcome of the violation is different for a certain race category than it is for others, such as if people in one race were more or less likely to be arrested. We will also aim to answer this in terms of if a certain violation category more frequently resulted in a type of outcome as well. A challenge that we may come across is in organizing the data since it looks like the data may contain repeats of the same violator when there were multiple records of a violation, whether the same or different type, for one person. An additional challenge that might come up when analyzing this dataset is from the missing values in some columns which could make it more difficult to see connections.\nPeople Receiving Homeless Response Services by Age, Race, Ethnicity, and Gender - Homelessness Demographic By Race https://catalog.data.gov/dataset/people-receiving-homeless-response-services-by-age-race-ethnicity-and-gender-b667d This data set has 5 columns and 2162 rows. The data was originally collected by 44 Continuums of Care (CoC) in California, which are regional planning bodies and service coordinators for homelessness care, and the data is about the people the centers serve to help them learn more about their demographics. The data is a CSV file so it can be loaded into R Studio and cleaned. We hope to address with this data set if one race receives more care for homelessness than other races across all the CoC centers in California, if there are any trends in the number of people of each race receiving care (is it increasing, decreasing, or steady), and if one region serves a larger population of homeless people than other regions. A challenge that might come up in the future is the data set does not have a lot of information so more complex analysis of the data might be limited. The data also seems to include age, ethnicity, and gender information but in different CSV files so we were wondering if there was a way for us to combine this information.\nNCHS - Death rates and life expectancy at birth https://catalog.data.gov/dataset/nchs-death-rates-and-life-expectancy-at-birth This data set has 5 columns and 1072 rows. FIve columns consist of Year, Race, Sex, Average Life Expectancy in Years, and Age-adjusted Death Rate. This dataset tracked U.S. mortality trends from 1900 to 2017. It highlights the differences in age-adjusted death rates and life expectancy at birth by race and sex. Using this dataset, we hope to discover any correlation between race and life expectancy. Of course, there are multiple factors that we cannot take into our consideration, but we believe that at least some kind of trend should be visible with R. As time progresses, the medical care people receive should have been better than before. This means life expectancy would go up, but that would vary among different races and there is a difference between who can receive the care and who cannot. It is true that it would have been better if there were more columns and rows; however, it would be interesting to extract good information from a small sample of data. If this data does not work, we hope to find better datasets from online."
  },
  {
    "objectID": "posts/2023-12-11-blog-post-6/blog-post-6.html",
    "href": "posts/2023-12-11-blog-post-6/blog-post-6.html",
    "title": "Blog Post 6",
    "section": "",
    "text": "For our model, we have chosen Arrest types as our binomial response variable, which ‘marked’ arrest type as 1 and ‘Unmarked’ arrest type as type 0. Our predictor variables are Race, Alcohol, and Gender. We then used a glm model for the binomial family.\nIn the original model we generated, we have observed that except the variable “male Gender”, every other categorical variables has significant effect in the prediction of probability of our dependent model, which points out that there is relation between the categorical variables we choose and their arrest type probability.\nWhat’s more, based on the coefficient value, we can summarize that the variables “Black”,“Hispanic”,“Other” and “white” in Categorical “Race”, the variable “Alcohol” has negative log-odds, which means that will lower the probability they got arrest type as “marked”. While the variables “Male” “U” in Categorical variable”Gender”, the Race “Native American” has positive log-odds, which will increase their probability they got the arrest type as “Marked”.\nWe also built a confusion matrix to find our model accuracy. From this, our model accuracy is 0.9277944, which roughly is %92.78, when we pick the predict probability larger than 0.5 as 1 and less than 0.5 as 0. Compare with the marked and unmarked arrest type proportion, we may cautiously evaluate this model’s accuracy because we have to think about the probability of overfitting in our mode and the imbalance in our model.\nIn the model summary, we have notices that the null deviance is 984980 and residual deviance is 979510, and the deviance range is (-2.6691,1.2755), and our model may contains over fitting because of the imbalance outcome proportion in our dependent variable.\nBased on the current outcome of our origional model, to enhance the performance of our original model, we are exploring several avenues for improvement. First and foremost, our current dataset comprises solely categorical variables. Introducing relevant numerical variables could significantly augment our model’s predictive capabilities, particularly when utilizing logistic regression as our analytical framework. To achieve this, we are considering merging our existing dataset with other correlated datasets, a step we believe is crucial in uncovering additional potential relationships between the dependent and independent variables.\nAdditionally, we recognize the imbalance in our current dependent variable. To address this, we are contemplating the exploration of alternative dependent variables. Building multiple analysis models with varied dependent variables can provide a broader perspective, potentially revealing different relationships and leading to more robust conclusions. This approach not only enhances the stability of our findings but also enriches our exploratory data analysis (EDA) by fostering new insights.\nBy diversifying our dataset and exploring alternative dependent variables, we aim to refine our understanding of the underlying relationships, ultimately improving the overall performance and reliability of our analytical models.\nWe plan to polish our visualizations and tables after finalizing our model to best show the data in terms of our method of statistical modeling. This will include highlighting information that informed the variables we made the decision to include. For example, in our scatterplot from our exploratory data analysis we will highlight the variables race, alcohol, and gender which seemed to have a potential relationship with arrest_type_numeric to show that this contributed to these variables being chosen for our statistical model. We also plan to add titles to our visualizations and tables to more clearly show what is being modeled in that particular plot.\nAdditionally, we will polish up our data visualizations by adding captions and annotations to write a short summary of what our takeaways from that visualization were. For example, in the scatterplot mentioned before we will add a caption or annotation saying that only race, alcohol, and gender appear to potentially have a relationship with arrest_type_numeric. We also plan to improve our figures using the options for displaying tables from https://gallery.htmlwidgets.org/, particularly the scatterD3 option to include both colors and comments more clearly for specific points in our plots. We are also planning to use pairsD3 to be able to show various relationships between variables that we explored to choose our model.\nWe are still trying out different EDA, with GLM being the priority. We are still unsatisfied with produced results, but we believe we could reach a conclusion that everyone can agree."
  },
  {
    "objectID": "posts/2023-11-06-blog-post-3/blog-post-3.html",
    "href": "posts/2023-11-06-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "library(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\nWarning: package 'stringr' was built under R version 4.3.1\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nload(\"C:/Users/pfwan/OneDrive/Desktop/MA 615/ma-4615-fa23-final-project-burg-team-9/dataset/traffic_violations.RData\")\nmydata &lt;- traffic_data_clean\n\nstate_counts &lt;- mydata %&gt;% \n  group_by(`DL State`) %&gt;%\n  summarise(count= n()) %&gt;% \n  arrange(desc(count))\n\nplot1 &lt;- ggplot(state_counts, aes(y=reorder(`DL State`, count),x=count)) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"The Count of DL state appear times\", x = \"DL State\", y = \"Count numbers\") + theme_minimal() +\n  theme(axis.text.y =element_text(size = 4)) +theme(plot.background = element_rect(fill = \"white\"))\nplot1\n\n\n\nggsave(\"blog plot1 .png\", plot1, width = 8, height = 4, units = \"in\")\n\n\nstate_percentage &lt;- mydata %&gt;% \n  group_by(`DL State`) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(percentage = (count / sum(count)) * 100) %&gt;%\n  arrange(desc(count)) %&gt;%\n  mutate(state_group = ifelse(percentage &lt; 1, \"other\",`DL State`)) %&gt;%\n  group_by(state_group) %&gt;%\n  summarise(count = sum(count), percentage = sum(percentage))\n\n\nplot2 &lt;- ggplot(state_percentage, aes(x = \"\", y = percentage, fill = state_group)) +\n  geom_bar(stat = \"identity\") +coord_polar(\"y\", start = 0) +\n  labs(title = \"State Count Percentages\", subtitle = \"All states have percentage less than 1 grouped as 'other'\",fill = \"State\") +   geom_text(aes(label = paste0(round(percentage, 1), \"%\")),position = position_stack(vjust = 0.5))+\n  theme_void()+theme(plot.background = element_rect(fill = \"white\"))\nplot2\n\n\n\nggsave(\"blog plot2 .png\", plot2, width = 8, height = 4, units = \"in\")\n\nAfter reviewing our initial subset of data, we decided to add back some columns we previously removed as we believe they could potentially have an interesting and significant correlation with race, and we had space to incorporate these columns while keeping our subset of data under 50 MB. The columns include: Time of stop, Accident, Fatal, Alcohol, Driver’s License State, and Model. During our basic exploratory data analysis we noticed that the Driver’s License State is predominantly Maryland. While this is an unusually high frequency for one state, in the context of our data it makes sense because our data was collected by the Montgomery County, Maryland Police department.\nAfter thorough discussion and analysis of our filtered data, we have identified a crucial area for investigation: the potential relationship between drivers’ race and their original state. This exploration has the potential to significantly benefit local government statistics, enhance their understanding of the local traffic landscape, and ultimately improve work efficiency. In order to achieve this goal, visualizing the driver state and evaluating the outliers and data distribution in this variable are the first steps we need to take.Thus, we have chosen to use DL State, representing the state of the driver’s home address, rather than the variable State, which denotes the state where the vehicle is registered as our plot parameter.\nDepending on the graph “blog plot1” we build, the data presented in this graph shows that Maryland (MD) has a significantly larger volume of data compared to any other state, even the combined data from all other states.Furthermore, the pie chart “blog plot2” that using the state percentage,where categorizes states with a percentage less than 1 as “other”, also shows that the percentage of MD is 87 percent, a very unusual high value. After discussion, we believe that the main reason for this phenomenon is that our dataset primarily consists of daily statistics from Montgomery County, Maryland, so a large portion of the data originating from Maryland residents is reasonable.\nAfter further research on how to clean up this part of the data, we believe that it should be handled differently based on the consideration of other variables, especially the distribution of race. If we focus our research on the race of Maryland natives,we can treat data from other states as outliers or unusual values and consider cleaning them up. However, if we focus on the involving out-of-state drivers race, the data from Maryland should be seen as a substantial influence on the model’s accurate representation of unusual outliers.Given that we have not fully explored their potential connections to other factors, their potential prior connections to other factors, and that neither the feasibility of other factors as model variable nor the data cleanup considerations have been fully completed, we have decided not to undertake data cleaning for the unusual values presented in this chart.\nIn our next step, some important topics will be discussed: the race proportion of driver’s and its relationship between Arrest type and State, and the potential linear relationship between Race, DL states and Violation Type. All data cleaning will be gradually determined and implemented in subsequent discussions."
  },
  {
    "objectID": "posts/2023-11-22-blog-post-5/blog-post-5.html",
    "href": "posts/2023-11-22-blog-post-5/blog-post-5.html",
    "title": "Blog Post 5",
    "section": "",
    "text": "Acknowledging our dataset’s origins in Montgomery’s daily traffic records, , If we want to find possible datasets to combine with, we believe that focusing on the data resource from different aspects of Montgomery is a good starting point. After primary discussion, We decided to identify location-related factors as the main combine factor objects. Government’s official dataset, replete with detailed location coordinates, longitude, latitude, and street names, will be the prime candidates for dataset combination.\nUpon thorough exploration, two datasets, Crash Reports surfaced as an ideal contender for combination. In previous discussions and studies, we decide to focus towards understanding the intricate relationship between arrest types and accident types, with race as a central analytical factor. The “Crash Report” dataset stood out due to its comprehensive details on accidents, including location specifics such as intersections, weather conditions, and collision types. This richness in data allows for a more nuanced construction of our predictive model by incorporating variables that influence accident types. Therefore, it becomes an ideal combined dataset. ( https://data.montgomerycountymd.gov/Public-Safety/Crash-Reporting-Incidents-Data/bhju-22kf )\nGiven that both databases utilize the same record format for data collected from identical locations, various sharing factors can be considered as combined factors. After experimenting with Latitude, Longitude, and Location, the decision was reached to use Latitude and Longitude as the primary integration factors, offering the most optimal analysis environment. This choice not only ensures seamless database merging but also guarantees analysis precision. From the perspective of data visualization, using location as a combining factor can help us to generate real-world latitude/longitude charts to help us visualize the frequency of occurrence of the arrest type and accident type in each region, which will bring us more three-dimensional analysis results.On the another perspective of linear modeling, more variables can be selected and integrated to help us add more variables to our linear model and increase the accuracy of the model.\nIn conclusion, by focusing on regionally relevant factors, combining the dataset Crash reports with our original is a valuable data consolidation option.This approach not only ensures comprehensive analysis but also lays the groundwork for a more detailed and accurate predictive model. The use of Latitude and Longitude as key integration factors enhances our analytical capabilities, allowing for more insightful data visualization and improved linear modeling accuracy. While we are having difficulties joining the data by both latitude and longitude, our plan going forward is to combine the two columns into one in order to use this to consolidate the two datasets. One of the main errors we are having trouble with is that there was an unexpected many to many relationship between x and y, which we will continue to figure out going forward. In subsequent analyses, we will also consider how more specific filtering and combining datasets can take advantage of these potential strengths, detailing our analysis. In the meantime, finding more suitable data that can be used as combining objects is also a necessary step to accomplish. So far we have also searched for several usable candidates such as the dataset Crime Report. More details will be agreed upon in subsequent discussions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Blog Post 7\n\n\nTentative Thesis, Evidence, and Next Steps\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 6\n\n\nCurrent Model Analysis and Next Steps\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 5\n\n\nSecond Dataset for Analysis and Combining Datasets\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 4\n\n\nExploratory Data Analysis and Initial Statistical Modeling\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 3\n\n\nData Cleaning and Unusal Value Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 2\n\n\nIntial Data Cleaning and Equitable Data Practice\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 1\n\n\nPotential Datasets for Analysis\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nTeam 9\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team BURG. The members of this team are below."
  },
  {
    "objectID": "about.html#woo-hyeon-luke-her",
    "href": "about.html#woo-hyeon-luke-her",
    "title": "About",
    "section": "Woo Hyeon (Luke) Her",
    "text": "Woo Hyeon (Luke) Her\nWoo Hyeon is an undergraduate student studying Data Science. www.github.com/365LIT"
  },
  {
    "objectID": "about.html#megha-polavarapu",
    "href": "about.html#megha-polavarapu",
    "title": "About",
    "section": "Megha Polavarapu",
    "text": "Megha Polavarapu\nMegha Polavarapu is an undergraduate student at Boston University studying Mathematics."
  },
  {
    "objectID": "about.html#holly-gustavsen",
    "href": "about.html#holly-gustavsen",
    "title": "About",
    "section": "Holly Gustavsen",
    "text": "Holly Gustavsen\nHolly Gustavsen is an undergraduate student studying Biology at BU."
  },
  {
    "objectID": "about.html#jaejoong-kim",
    "href": "about.html#jaejoong-kim",
    "title": "About",
    "section": "Jaejoong Kim",
    "text": "Jaejoong Kim\nJaejoong Kim is an undergraduate student studying Data Science and Business at BU."
  },
  {
    "objectID": "about.html#pengfei-wang",
    "href": "about.html#pengfei-wang",
    "title": "About",
    "section": "Pengfei Wang",
    "text": "Pengfei Wang\nPengfei Wang is a graduate student studying Statistics at Boston University.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you hope to ask, illustrations relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the techniques you used for validating your results.\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, use clean easy-to-read code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\n\nDriving Under the Lens: Uncovering the Truth Behind Racial Disparities in Montgomery County’s Traffic Arrests\nGeorge Floyd, Eric Garner, and Michael Brown are only a few examples of the fatal incidents that highlight a dangerous prejudice persisting in the United States. The United States is composed of countless different groups made up of different identities. A significant part of these identities is race. Just within this one country, there are six racial categories, as well as people who belong to more than one race. Yet, each of these groups is looked at differently. The United States has been seen to give rise to both prominent and unconscious biases. While always present, in recent times, there has been more news regarding cases of systemic racism as seen with Floyd, Garner, and Brown. This rise in attention towards racial inequality gives rise to the question of what influences are contributing to biases, especially on how police systems may show patterns behind the variations seen in the outcomes for different races.\nThis analysis focuses on these trends, what may be influencing them, and how they then can be targeted. We focus on the police system through traffic violations specifically in the area of Montgomery County, Maryland, by analyzing information such as the types of violations, the arrest characteristics, and locations. This approach aims to uncover if and how these disparities are due to systemic issues.\n\n\nThesis For Analysis\nTraffic reports can have a variety of influences both from factors included in the data set and ones outside of it. To understand the possibilities of what is driving patterns in traffic violations, we made use of influential factors that were included in our dataset and appeared influential in our exploratory data analysis. These primarily included race, gender, if alcohol was involved in the accident, and if the violation was a car crash or a traffic violation incident. Using two datasets from Montgomery County’s records on traffic violations and these factors, we aimed to develop a method of modeling that would be capable of predicting accident probabilities based on these influences. This model revealed important patterns in the probabilities of accidents in Montgomery County to develop the thesis of our analysis: non-white populations of Montgomery County, Maryland have more instances of traffic violations when compared to areas of the county that are predominantly white.\nTo research this thesis, we aimed to analyze the potential relationships between traffic violations and accidents and race. The racial categories we worked with were six categories, Asian, White, Black, Hispanic, Native American, and Other. Each of these groups displayed different rates and characteristics of accidents throughout our exploratory data analysis. This was primarily shown by the visualizations below, showing the relationships between race and other variables given by our datasets. The first chart showcases the proportions of gender that were recorded to have traffic violations for each race, with the blue bars representing males and the red ones showing females. In general for each race, the larger proportion of traffic violations were for males, but the proportions did differ by race. The second chart shows the proportion of alcohol use for each race. Each race has one bar, which is colored partly blue to represent the proportion of violations that involved alcohol, and red to show the ones that did not involve alcohol use. This chart showed that the majority of traffic violations for this county did not involve alcohol use, but the proportion that did not involve alcohol did vary by race. These exploratory plots show that there may be potential relationships between race and gender as well as between race and alcohol use, indicating that including gender and alcohol use as predictors could be relevant.\nThe pie chart below shows the categories of violations colored to show the relative proportions of the data that fall into each category. Because there are differences between the categories, it additionally indicates that violation type may be informative in discovering patterns within the traffic violations to connect our predictions to.\n\n\nVisualizations\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis Explanation\nTherefore, to discover more valuable information, we additionally expanded our analysis to include race, gender, if alcohol use was involved in the accident, and the type of violation that the incident was to develop a more overarching understanding of the patterns in the datasets to provide valuable insights behind Montgomery County’s accident probabilities and their influences. These inclusions were also aimed to uncover any potential influences working in combination with race to influence accidents in the county.\nThe findings resulting from our analyses provide insights into the way that Montgomery County’s police and government systems work, and where there are limitations in the equality of their current practices, thus pointing to where improvements could be made. The information revealed from the analysis presents Montgomery County, Maryland with a better awareness of how they may be able to prevent future accidents in common areas or within common groups or reduce their likelihoods through the examination of influential factors. This gives the county the knowledge to begin considering and addressing biases that are present within their departments and decrease the consequences. The disproportional effects in different areas and groups of people shows a common issue of inequality, a danger that can motivate Montgomery County to begin implementing changes. By addressing the key issues we have identified, the county’s police department and government systems can have a strong starting point on how to address these matters. With this, Montgomery County can investigate further into the general patterns and trends uncovered in terms of analyzing more contributing factors and consequences while broadening the scope of where and how systemic racism’s effects can be seen.\n\n\nGoals\nOur study aims to delve into the relationship between race and systemic inequality, while maintaining responsible data analysis, objective findings, and considering social implications. Our goal is to provide insights based on data analysis specifically regarding the disparities that the communities of Montgomery County, Maryland face in their daily lives as they commute from place to place. We aim to inform more equitable practices within police and government systems and contribute to more equitable systems. This analysis is a step towards understanding and addressing the complex realities of racial disparities in traffic violations, paving the way for future improvements and further research."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "[] (https://www.montgomerycountymd.gov/mcgresponsive/Resources/Images/countyseal.png)\nThe data was originally found on Kaggle.com and then source data was later found on the following dataMontomery website. This data is updated frequently, our dataset is from November 3rd, 2023. The data was originally collected by the Montgomery County Police Station in Maryland. It was collected for the purpose of observing drug and traffic violations and creating a government and police record of violations. It was put together using traffic violation information from all electronic traffic violations issued in Montgomery County.\nThe data file that was used was originally in the form of a CSV file. This original dataset had 43 variables, but because it was large, we chose to use only the more relevant 14 variables in our cleaned data set, which are Date of stop, Time of stop, Description, Location, Accident, Fatal, Alcohol, Driver’s License State, Year, Make, Model, Violation type, Race, and Arrest type. Description shows what the specific charge was, such as exceeding the posted speed limit. Location indicates the specific street in Montgomery County where the violation occurred, while Driver’s License State shows the state where the driver is from, which is not necessarily Maryland. Make and Model refers to the car that was being driven. Fatal and Alcohol only say Yes or No, while Violation Type and Arrest Type provide more specific descriptions of the violation that occurred and its outcome, respectively.\nFor preliminary cleaning, we removed columns that we believe would not be helpful or relevant in our data analysis in our cleaning script. Some examples of this are latitude and longitude coordinates. No additional R packages were requrired for this and no other data sets were merged with the current data."
  }
]