[
  {
    "objectID": "posts/2023-11-20-blog-post-5/blog-post-5.html",
    "href": "posts/2023-11-20-blog-post-5/blog-post-5.html",
    "title": "Blog Post 5",
    "section": "",
    "text": "Acknowledging our dataset’s origins in Montgomery’s daily traffic records, , If we want to find possible datasets to combine with, we believe that focusing on the data resource from different aspects of Montgomery is a good starting point. After primary discussion, We decided to identify location-related factors as the main combine factor objects. Government’s official dataset, replete with detailed location coordinates, longitude, latitude, and street names, will be the prime candidates for dataset combination.\nUpon thorough exploration, two datasets, Crash Reports surfaced as an ideal contender for combination. In previous discussions and studies, we decide to focus towards understanding the intricate relationship between arrest types and accident types, with race as a central analytical factor. The “Crash Report” dataset stood out due to its comprehensive details on accidents, including location specifics such as intersections, weather conditions, and collision types. This richness in data allows for a more nuanced construction of our predictive model by incorporating variables that influence accident types. Therefore, it becomes an ideal combined dataset. ( https://data.montgomerycountymd.gov/Public-Safety/Crash-Reporting-Incidents-Data/bhju-22kf )\nGiven that both databases utilize the same record format for data collected from identical locations, various sharing factors can be considered as combined factors. After experimenting with Latitude, Longitude, and Location, the decision was reached to use Latitude and Longitude as the primary integration factors, offering the most optimal analysis environment. This choice not only ensures seamless database merging but also guarantees analysis precision. From the perspective of data visualization, using location as a combining factor can help us to generate real-world latitude/longitude charts to help us visualize the frequency of occurrence of the arrest type and accident type in each region, which will bring us more three-dimensional analysis results.On the another perspective of linear modeling, more variables can be selected and integrated to help us add more variables to our linear model and increase the accuracy of the model.\nIn conclusion, by focusing on regionally relevant factors, combining the dataset Crash reports with our original is a valuable data consolidation option.This approach not only ensures comprehensive analysis but also lays the groundwork for a more detailed and accurate predictive model. The use of Latitude and Longitude as key integration factors enhances our analytical capabilities, allowing for more insightful data visualization and improved linear modeling accuracy. While we are having difficulties joining the data by both latitude and longitude, our plan going forward is to combine the two columns into one in order to use this to consolidate the two datasets. One of the main errors we are having trouble with is that there was an unexpected many to many relationship between x and y, which we will continue to figure out going forward. In subsequent analyses, we will also consider how more specific filtering and combining datasets can take advantage of these potential strengths, detailing our analysis. In the meantime, finding more suitable data that can be used as combining objects is also a necessary step to accomplish. So far we have also searched for several usable candidates such as the dataset Crime Report. More details will be agreed upon in subsequent discussions."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post.\nEvery time you want to make a new post, you can repeat step 2 above. When you want to publish your progress, follow steps 4-7 from Customize your site.\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2023-10-30-blog-post-2/blog-post-2.html",
    "href": "posts/2023-10-30-blog-post-2/blog-post-2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "Since we are working with a large data set, we needed to start with only a subset of the data. We chose this subset by including columns without common instances of missing data. We also focused on choosing columns with different variables instead of columns that generally had the same response for each row in order to be able to see more variation in patterns and trends. This managed to create a subset that is now 34.6 MB so it will be more efficient to work with, but we will be able to add in more data later in the process if needed.\nThe principles for advancing equitable data practice that are relevant to our data are mainly beneficence and justice. Beneficence is relevant because it involves data that is collected with identification. Adhering to this practice involves removing anything in our data that could be used to identify people. In order to this, we remove any violation numbers or record numbers that could be traced back to a specific person. Justice is relevant because it involves considering the community in our design interest. Adhering to this practice involves us considering how the specific community from which the data comes from will impact our analysis and keeping that in mind instead of generalizing our data.\nOne of the important principles that were discussed was transparency. In order to implement this, we will need to be transparent both about what the limits of the data we have chosen are and what data informs our analysis. This will involve documenting our process clearly along the way in terms of how we are making this analysis. Therefore, some limitations of our analysis will be that we can only create analysis in terms of the specific community that our data comes from rather than generalizing it. We will also have to be aware that there may have been biases in how the data was collected so it may not be completely accurate or representative. For example, the data recorded at night may have been less accurate than the observations collected during the day. Additionally, observations may have been more likely to be recorded for certain types of violations, for example, ones that were worse, or for certain race categories.\nTransparency also involves being clear and transparent about what the plans are for the data after the project concludes. This refers to the potential for abuse or misuse of the data if we are not cautious about who has access to the data both during and after the project is completed."
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (Next week, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext week, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2023-11-13-blog-post-4/blog-post-4.html",
    "href": "posts/2023-11-13-blog-post-4/blog-post-4.html",
    "title": "Blog Post 4",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nload(here::here(\"dataset/traffic_violations.RData\"))\n \nwd &lt;- traffic_data_clean %&gt;% \n  rename(Arrest_Type = `Arrest Type`)\n \n# Race vs Arrest Type\nggplot(wd, aes(x = Race, fill = Arrest_Type)) + \n  geom_bar(position = \"dodge\") + \n  labs(title = \"Distribution of Race by Arrest Type\", \n       x = \"Race\", y = \"Count\",\n       fill = \"Arrest Type\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\nggplot(wd, aes(x = Race, fill = Accident)) + \n  geom_bar(position = \"dodge\") + \n  labs(title = \"Distribution of Race by Accident\", \n       x = \"Race\", y = \"Count\",\n       fill = \"Accident\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n# Statistical Modeling\n\nlibrary(tidyverse)\n\ntraffic_data_clean &lt;- traffic_data_clean |&gt;\n  mutate(Race_binary = ifelse(Race == \"Hispanic\", 1, 0))\n\nfiltered_data &lt;- traffic_data_clean |&gt;\n  filter(Year &gt;= 2000 & Year &lt;= 2023) |&gt;\n  mutate(Numbered_Year = Year - 2000)\n\nmodel &lt;- glm(Race_binary ~ Numbered_Year, family = binomial, data = filtered_data)\n\nWarning: glm.fit: algorithm did not converge\n\nggplot(filtered_data, aes(x = Numbered_Year, y = Race_binary)) +\n  geom_point() +\n  geom_smooth(method = 'glm', formula = y ~ x, se = FALSE, color = \"blue\", size = 0.5) +\n  labs(title = \"Logistic Regression: Race_binary ~ Numbered_Year\",\n       x = \"Numbered_Year\",\n       y = \"Race_binary\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nFrom our last blog post, we saw that one of the most big, obvious trends was that the Driver’s License State category was predominantly from Maryland. Since this is to be expected given that the data is from a county in Maryland, this may be a boring part of the data. So, we chose to explore some other relationships that might be confounded by this major trend we identified.\nTo explore our data further in this way, we first looked at relationships among different variables and groupings by looking for trends or patterns between race categories and arrest types as well as race categories and accident types. We then focused on developing a better understanding of the specific relationship between race and arrest types and race and accident types by also including the time of day to see if this too affects the outcome of the traffic violation.\nBeginning to think about statistical modeling, the response variable that we are interested in is the outcome of the traffic violation, which in our data is represented as arrest type and accident type. The predictor variables we are interested in are race and time of day. To include time of day we would need to perform some transformations to better capture the relationships between the variables. Since time is going in a loop as it goes to the end of the day and then the values go back down to 0, we need to transform this column of the data to be continuous instead of a cycle. We will fit a linear model so that we can see how the predictor variables affect the response variable. For this, we will also need another transformation to represent the outcome of the traffic violation as binary variables. For the arrest type, we can choose “A – Marked Patrol” to be represented by 1, and each other arrest type to be 0. Similarly, for the accident column, we can choose “Yes” to be 1 and “No” to be 0. After these transformations, we can use the categorical variables in the linear model.\nFor our initial statistical modeling, we began by using a logistic model with the race as the response variable and the year as the predictor variable to observe the potential relationships between the differences in race categories reported for traffic violations over a number of years. For this initial modeling, we transformed the race variable to be binary by mutating “Hispanic” to be 1 and every other race category to be 0. We also transformed the Year variable to be represented as a value showing how many years from 2000 it is. We also only included the years 2000-2023 to first see how we could analyze a subset of the data on a smaller scale. For our future statistical modeling, we will also include the other race categories to see the trends and patterns between the different categories and include more years to see more widespread trends. This initial model showed a straight horizontal line after the regression, so in our further analysis we will additionally add more variables to find a model that reveals more about the data and its trends."
  },
  {
    "objectID": "posts/2023-10-20-blog-post-1/blog-post-1.html",
    "href": "posts/2023-10-20-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "Traffic and Drugs Related Violations Dataset https://www.kaggle.com/datasets/shubamsumbria/traffic-violations-dataset/data This data set has 15 columns and 52,967 rows. The data was originally collected to record observations of traffic and drugs violations. This dataset is in the form of a CSV file, so we will be able to load and clean the data. The main question we hope to address is if one race has more traffic and drugs related violations than the other race categories. Another question we hope to address is if the outcome of the violation is different for a certain race category than it is for others, such as if people in one race were more or less likely to be arrested. We will also aim to answer this in terms of if a certain violation category more frequently resulted in a type of outcome as well. A challenge that we may come across is in organizing the data since it looks like the data may contain repeats of the same violator when there were multiple records of a violation, whether the same or different type, for one person. An additional challenge that might come up when analyzing this dataset is from the missing values in some columns which could make it more difficult to see connections.\nPeople Receiving Homeless Response Services by Age, Race, Ethnicity, and Gender - Homelessness Demographic By Race https://catalog.data.gov/dataset/people-receiving-homeless-response-services-by-age-race-ethnicity-and-gender-b667d This data set has 5 columns and 2162 rows. The data was originally collected by 44 Continuums of Care (CoC) in California, which are regional planning bodies and service coordinators for homelessness care, and the data is about the people the centers serve to help them learn more about their demographics. The data is a CSV file so it can be loaded into R Studio and cleaned. We hope to address with this data set if one race receives more care for homelessness than other races across all the CoC centers in California, if there are any trends in the number of people of each race receiving care (is it increasing, decreasing, or steady), and if one region serves a larger population of homeless people than other regions. A challenge that might come up in the future is the data set does not have a lot of information so more complex analysis of the data might be limited. The data also seems to include age, ethnicity, and gender information but in different CSV files so we were wondering if there was a way for us to combine this information.\nNCHS - Death rates and life expectancy at birth https://catalog.data.gov/dataset/nchs-death-rates-and-life-expectancy-at-birth This data set has 5 columns and 1072 rows. FIve columns consist of Year, Race, Sex, Average Life Expectancy in Years, and Age-adjusted Death Rate. This dataset tracked U.S. mortality trends from 1900 to 2017. It highlights the differences in age-adjusted death rates and life expectancy at birth by race and sex. Using this dataset, we hope to discover any correlation between race and life expectancy. Of course, there are multiple factors that we cannot take into our consideration, but we believe that at least some kind of trend should be visible with R. As time progresses, the medical care people receive should have been better than before. This means life expectancy would go up, but that would vary among different races and there is a difference between who can receive the care and who cannot. It is true that it would have been better if there were more columns and rows; however, it would be interesting to extract good information from a small sample of data. If this data does not work, we hope to find better datasets from online."
  },
  {
    "objectID": "posts/2023-11-06-blog-post-3/blog-post-3.html",
    "href": "posts/2023-11-06-blog-post-3/blog-post-3.html",
    "title": "Blog Post 3",
    "section": "",
    "text": "library(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.3.1\n\n\nWarning: package 'dplyr' was built under R version 4.3.1\n\n\nWarning: package 'stringr' was built under R version 4.3.1\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nload(\"C:/Users/pfwan/OneDrive/Desktop/MA 615/ma-4615-fa23-final-project-burg-team-9/dataset/traffic_violations.RData\")\nmydata &lt;- traffic_data_clean\n\nstate_counts &lt;- mydata %&gt;% \n  group_by(`DL State`) %&gt;%\n  summarise(count= n()) %&gt;% \n  arrange(desc(count))\n\nplot1 &lt;- ggplot(state_counts, aes(y=reorder(`DL State`, count),x=count)) + \n  geom_bar(stat = \"identity\") +\n  labs(title = \"The Count of DL state appear times\", x = \"DL State\", y = \"Count numbers\") + theme_minimal() +\n  theme(axis.text.y =element_text(size = 4)) +theme(plot.background = element_rect(fill = \"white\"))\nplot1\n\n\n\nggsave(\"blog plot1 .png\", plot1, width = 8, height = 4, units = \"in\")\n\n\nstate_percentage &lt;- mydata %&gt;% \n  group_by(`DL State`) %&gt;%\n  summarise(count = n()) %&gt;%\n  mutate(percentage = (count / sum(count)) * 100) %&gt;%\n  arrange(desc(count)) %&gt;%\n  mutate(state_group = ifelse(percentage &lt; 1, \"other\",`DL State`)) %&gt;%\n  group_by(state_group) %&gt;%\n  summarise(count = sum(count), percentage = sum(percentage))\n\n\nplot2 &lt;- ggplot(state_percentage, aes(x = \"\", y = percentage, fill = state_group)) +\n  geom_bar(stat = \"identity\") +coord_polar(\"y\", start = 0) +\n  labs(title = \"State Count Percentages\", subtitle = \"All states have percentage less than 1 grouped as 'other'\",fill = \"State\") +   geom_text(aes(label = paste0(round(percentage, 1), \"%\")),position = position_stack(vjust = 0.5))+\n  theme_void()+theme(plot.background = element_rect(fill = \"white\"))\nplot2\n\n\n\nggsave(\"blog plot2 .png\", plot2, width = 8, height = 4, units = \"in\")\n\nAfter reviewing our initial subset of data, we decided to add back some columns we previously removed as we believe they could potentially have an interesting and significant correlation with race, and we had space to incorporate these columns while keeping our subset of data under 50 MB. The columns include: Time of stop, Accident, Fatal, Alcohol, Driver’s License State, and Model. During our basic exploratory data analysis we noticed that the Driver’s License State is predominantly Maryland. While this is an unusually high frequency for one state, in the context of our data it makes sense because our data was collected by the Montgomery County, Maryland Police department.\nAfter thorough discussion and analysis of our filtered data, we have identified a crucial area for investigation: the potential relationship between drivers’ race and their original state. This exploration has the potential to significantly benefit local government statistics, enhance their understanding of the local traffic landscape, and ultimately improve work efficiency. In order to achieve this goal, visualizing the driver state and evaluating the outliers and data distribution in this variable are the first steps we need to take.Thus, we have chosen to use DL State, representing the state of the driver’s home address, rather than the variable State, which denotes the state where the vehicle is registered as our plot parameter.\nDepending on the graph “blog plot1” we build, the data presented in this graph shows that Maryland (MD) has a significantly larger volume of data compared to any other state, even the combined data from all other states.Furthermore, the pie chart “blog plot2” that using the state percentage,where categorizes states with a percentage less than 1 as “other”, also shows that the percentage of MD is 87 percent, a very unusual high value. After discussion, we believe that the main reason for this phenomenon is that our dataset primarily consists of daily statistics from Montgomery County, Maryland, so a large portion of the data originating from Maryland residents is reasonable.\nAfter further research on how to clean up this part of the data, we believe that it should be handled differently based on the consideration of other variables, especially the distribution of race. If we focus our research on the race of Maryland natives,we can treat data from other states as outliers or unusual values and consider cleaning them up. However, if we focus on the involving out-of-state drivers race, the data from Maryland should be seen as a substantial influence on the model’s accurate representation of unusual outliers.Given that we have not fully explored their potential connections to other factors, their potential prior connections to other factors, and that neither the feasibility of other factors as model variable nor the data cleanup considerations have been fully completed, we have decided not to undertake data cleaning for the unusual values presented in this chart.\nIn our next step, some important topics will be discussed: the race proportion of driver’s and its relationship between Arrest type and State, and the potential linear relationship between Race, DL states and Violation Type. All data cleaning will be gradually determined and implemented in subsequent discussions."
  },
  {
    "objectID": "posts/2023-11-22-blog-post-5/blog-post-5.html",
    "href": "posts/2023-11-22-blog-post-5/blog-post-5.html",
    "title": "Blog Post 5",
    "section": "",
    "text": "Acknowledging our dataset’s origins in Montgomery’s daily traffic records, , If we want to find possible datasets to combine with, we believe that focusing on the data resource from different aspects of Montgomery is a good starting point. After primary discussion, We decided to identify location-related factors as the main combine factor objects. Government’s official dataset, replete with detailed location coordinates, longitude, latitude, and street names, will be the prime candidates for dataset combination.\nUpon thorough exploration, two datasets, Crash Reports surfaced as an ideal contender for combination. In previous discussions and studies, we decide to focus towards understanding the intricate relationship between arrest types and accident types, with race as a central analytical factor. The “Crash Report” dataset stood out due to its comprehensive details on accidents, including location specifics such as intersections, weather conditions, and collision types. This richness in data allows for a more nuanced construction of our predictive model by incorporating variables that influence accident types. Therefore, it becomes an ideal combined dataset. ( https://data.montgomerycountymd.gov/Public-Safety/Crash-Reporting-Incidents-Data/bhju-22kf )\nGiven that both databases utilize the same record format for data collected from identical locations, various sharing factors can be considered as combined factors. After experimenting with Latitude, Longitude, and Location, the decision was reached to use Latitude and Longitude as the primary integration factors, offering the most optimal analysis environment. This choice not only ensures seamless database merging but also guarantees analysis precision. From the perspective of data visualization, using location as a combining factor can help us to generate real-world latitude/longitude charts to help us visualize the frequency of occurrence of the arrest type and accident type in each region, which will bring us more three-dimensional analysis results.On the another perspective of linear modeling, more variables can be selected and integrated to help us add more variables to our linear model and increase the accuracy of the model.\nIn conclusion, by focusing on regionally relevant factors, combining the dataset Crash reports with our original is a valuable data consolidation option.This approach not only ensures comprehensive analysis but also lays the groundwork for a more detailed and accurate predictive model. The use of Latitude and Longitude as key integration factors enhances our analytical capabilities, allowing for more insightful data visualization and improved linear modeling accuracy. While we are having difficulties joining the data by both latitude and longitude, our plan going forward is to combine the two columns into one in order to use this to consolidate the two datasets. One of the main errors we are having trouble with is that there was an unexpected many to many relationship between x and y, which we will continue to figure out going forward. In subsequent analyses, we will also consider how more specific filtering and combining datasets can take advantage of these potential strengths, detailing our analysis. In the meantime, finding more suitable data that can be used as combining objects is also a necessary step to accomplish. So far we have also searched for several usable candidates such as the dataset Crime Report. More details will be agreed upon in subsequent discussions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Blog Post 5\n\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 5\n\n\nData Combination Discussion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 4\n\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 3\n\n\nData cleaning part 2 and Unusal value analyzation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 2\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 1\n\n\n\n\n\n\n\n\n\n\n\n\nOct 20, 2023\n\n\nTeam 9\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nOct 15, 2023\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nOct 13, 2023\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team BURG. The members of this team are below."
  },
  {
    "objectID": "about.html#woo-hyeon-luke-her",
    "href": "about.html#woo-hyeon-luke-her",
    "title": "About",
    "section": "Woo Hyeon (Luke) Her",
    "text": "Woo Hyeon (Luke) Her\nWoo Hyeon is an undergraduate student studying Data Science. www.github.com/365LIT"
  },
  {
    "objectID": "about.html#megha-polavarapu",
    "href": "about.html#megha-polavarapu",
    "title": "About",
    "section": "Megha Polavarapu",
    "text": "Megha Polavarapu\nMegha Polavarapu is an undergraduate student at Boston University studying Mathematics."
  },
  {
    "objectID": "about.html#holly-gustavsen",
    "href": "about.html#holly-gustavsen",
    "title": "About",
    "section": "Holly Gustavsen",
    "text": "Holly Gustavsen\nHolly Gustavsen is an undergraduate student studying Biology at BU."
  },
  {
    "objectID": "about.html#jaejoong-kim",
    "href": "about.html#jaejoong-kim",
    "title": "About",
    "section": "Jaejoong Kim",
    "text": "Jaejoong Kim\nJaejoong Kim is an undergraduate student studying Data Science and Business at BU."
  },
  {
    "objectID": "about.html#pengfei-wang",
    "href": "about.html#pengfei-wang",
    "title": "About",
    "section": "Pengfei Wang",
    "text": "Pengfei Wang\nPengfei Wang is a graduate student studying Statistics at Boston University.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you hope to ask, illustrations relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nprint(getwd())\n\n[1] \"/Users/megha/415 Final Project\"\n\ndata &lt;- read_csv(here::here(\"dataset/loan_refusal_clean.csv\"))\n\nRows: 20 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): bank\ndbl (4): min, white, himin, hiwhite\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nload(here::here(\"dataset/loan_refusal.RData\"))\nprint(ls())\n\n[1] \"data\"            \"has_annotations\" \"loan_data_clean\""
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the techniques you used for validating your results.\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, use clean easy-to-read code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Importantly, these should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should include a link to an interactive dashboard. The dashboard should be created either using Shiny or FlexDashboard (or another tool with professor’s approval). This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc.\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions from the Big Picture? Plotly with default hover text will get no credit. Be creative!\n\n\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "The data was originally found on Kaggle.com and then source data was later found on the following dataMontomery website. This data is updated frequently, our dataset is from November 3rd, 2023. The data was originally collected by the Montgomery County Police Station in Maryland. It was collected for the purpose of observing drug and traffic violations and creating a government and police record of violations. It was put together using traffic violation information from all electronic traffic violations issued in Montgomery County.\nThe data file that was used was originally in the form of a CSV file. This original dataset had 43 variables, but because it was large, we chose to use only the more relevant 14 variables in our cleaned data set, which are Date of stop, Time of stop, Description, Location, Accident, Fatal, Alcohol, Driver’s License State, Year, Make, Model, Violation type, Race, and Arrest type. Description shows what the specific charge was, such as exceeding the posted speed limit. Location indicates the specific street in Montgomery County where the violation occurred, while Driver’s License State shows the state where the driver is from, which is not necessarily Maryland. Make and Model refers to the car that was being driven. Fatal and Alcohol only say Yes or No, while Violation Type and Arrest Type provide more specific descriptions of the violation that occurred and its outcome, respectively.\nFor preliminary cleaning, we removed columns that we believe would not be helpful or relevant in our data analysis in our cleaning script. Some examples of this are latitude and longitude coordinates. No additional R packages were requrired for this and no other data sets were merged with the current data."
  }
]